{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a6fe73-75f2-4975-b9d4-42175b8e0cbb",
   "metadata": {},
   "source": [
    "Install the required packages by running the following command in the first cell of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4689eab9-8126-4a00-a525-ff67802f0da7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting pgvector\n",
      "  Downloading pgvector-0.4.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pgvector) (2.1.3)\n",
      "Downloading pgvector-0.4.1-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: pgvector\n",
      "Successfully installed pgvector-0.4.1\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain_google_vertexai\n",
      "  Downloading langchain_google_vertexai-2.0.24-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
      "  Downloading langchain_core-0.3.62-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.43-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.11.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting bottleneck<2.0.0,>=1.4.2 (from langchain_google_vertexai)\n",
      "  Downloading bottleneck-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting google-cloud-aiplatform<2.0.0,>=1.92.0 (from langchain_google_vertexai)\n",
      "  Downloading google_cloud_aiplatform-1.95.0-py2.py3-none-any.whl.metadata (35 kB)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from langchain_google_vertexai) (2.19.0)\n",
      "Collecting httpx<0.29.0,>=0.28.0 (from langchain_google_vertexai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_google_vertexai)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.6 (from langchain_google_vertexai)\n",
      "  Downloading numexpr-2.10.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: pyarrow<20.0.0,>=19.0.1 in /opt/conda/lib/python3.10/site-packages (from langchain_google_vertexai) (19.0.1)\n",
      "Collecting validators<1,>=0.22.0 (from langchain_google_vertexai)\n",
      "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.32.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (2.24.2)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (2.38.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (3.20.3)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (3.31.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (1.14.2)\n",
      "Requirement already satisfied: shapely<3.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (2.0.7)\n",
      "Collecting google-genai<2.0.0,>=1.0.0 (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai)\n",
      "  Downloading google_genai-1.17.0-py3-none-any.whl.metadata (35 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (4.13.0)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (0.16)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (1.7.1)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (3.10)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<0.29.0,>=0.28.0->langchain_google_vertexai)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (1.69.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (1.49.0rc1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (4.9)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (2.9.0.post0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (0.14.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (15.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (1.3.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (0.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain_google_vertexai) (1.17.0)\n",
      "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_google_vertexai-2.0.24-py3-none-any.whl (100 kB)\n",
      "Downloading transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading bottleneck-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (357 kB)\n",
      "Downloading google_cloud_aiplatform-1.95.0-py2.py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading huggingface_hub-0.32.2-py3-none-any.whl (509 kB)\n",
      "Downloading langchain_core-0.3.62-py3-none-any.whl (438 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.43-py3-none-any.whl (361 kB)\n",
      "Downloading numexpr-2.10.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (397 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
      "Downloading google_genai-1.17.0-py3-none-any.whl (199 kB)\n",
      "Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: validators, safetensors, regex, orjson, numexpr, httpx-sse, hf-xet, h11, bottleneck, async-timeout, requests-toolbelt, huggingface-hub, httpcore, tokenizers, httpx, transformers, langsmith, google-genai, langchain-core, langchain-text-splitters, google-cloud-aiplatform, langchain_google_vertexai, langchain\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 5.0.1\n",
      "    Uninstalling async-timeout-5.0.1:\n",
      "      Successfully uninstalled async-timeout-5.0.1\n",
      "  Attempting uninstall: requests-toolbelt\n",
      "    Found existing installation: requests-toolbelt 0.10.1\n",
      "    Uninstalling requests-toolbelt-0.10.1:\n",
      "      Successfully uninstalled requests-toolbelt-0.10.1\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.86.0\n",
      "    Uninstalling google-cloud-aiplatform-1.86.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.86.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed async-timeout-4.0.3 bottleneck-1.5.0 google-cloud-aiplatform-1.95.0 google-genai-1.17.0 h11-0.16.0 hf-xet-1.1.2 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.0 huggingface-hub-0.32.2 langchain-0.3.25 langchain-core-0.3.62 langchain-text-splitters-0.3.8 langchain_google_vertexai-2.0.24 langsmith-0.3.43 numexpr-2.10.2 orjson-3.10.18 regex-2024.11.6 requests-toolbelt-1.0.0 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.52.3 validators-0.35.0\n",
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.95.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.24.2)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.38.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (24.2)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.19.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.31.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.14.2)\n",
      "Requirement already satisfied: shapely<3.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.7)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.17.0)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.11.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (4.13.0)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.69.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.49.0rc1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.14.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=1.32.0->google-cloud-aiplatform) (1.7.1)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (4.9.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (0.28.1)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (15.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0->google-cloud-aiplatform) (2.1.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (1.3.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (0.16.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.26.20)\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Downloading psycopg2_binary-2.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.10\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (3.20.3)\n",
      "Requirement already satisfied: shapely in /opt/conda/lib/python3.10/site-packages (2.0.7)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas\n",
    "!pip install pgvector\n",
    "!pip install langchain langchain_google_vertexai transformers\n",
    "!pip install google-cloud-aiplatform\n",
    "!pip install psycopg2-binary\n",
    "!pip install protobuf\n",
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01afdbab-ab82-469d-9253-879a88485291",
   "metadata": {},
   "source": [
    "To use the newly installed packages in this Jupyter runtime, it is recommended to restart the runtime. **Restart the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531ee525-1c76-4a25-9470-b4fe104d17a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25216eaa-e870-49f7-b968-522e87631aaf",
   "metadata": {},
   "source": [
    "# Download and load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6857d73-9c00-4246-a3c1-cd2659ff1c7a",
   "metadata": {},
   "source": [
    "An AlloyDB cluster named cymbal-alloy-cluster is configured in this lab. To begin, let's locate the AlloyDB cluster's IP address.\n",
    "\n",
    "On the Google Cloud console title bar, type \"AlloyDB\" in the Search field, then click AlloyDB in the Products & Pages section.\n",
    "\n",
    "Locate the cluster named cymbal-alloy-cluster, and the primary instance named cymbal-master-instance. The private IP address of this instance serves as your access point for utilizing AlloyDB throughout the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35c349-f913-4137-b98c-5c6791a62304",
   "metadata": {},
   "source": [
    "import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebe89d4d-f47f-4b3b-b241-0bb2939c207d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "import vertexai\n",
    "\n",
    "PROJECT_ID = \"qwiklabs-gcp-01-0205e348a520\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c147b53-9fdf-46b3-a332-efe841df15ce",
   "metadata": {},
   "source": [
    "Run the following code snippet to import the `psycopg2` library, which allows Python to interact with PostgreSQL databases, reads the CSV dataset into a pandas DataFrame, and finally saves the DataFrame to a table named products in the AlloyDB cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa44f7d5-926e-4943-a28d-317487c1662c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully!\n",
      "('74a695e3675efc2aad11ed73c46db29b', 'Slip N Slide Triple Racer with Slide Boogies', 'Triple Racer Slip and Slide with Boogie Boards. The unit is 16 foot long. The unit has 3 sliding lanes.', 37.21)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Replace with your AlloyDB cluster credentials\n",
    "cluster_ip_address = \"10.12.0.2\"\n",
    "database_user = \"postgres\"\n",
    "database_password = \"postgres\"\n",
    "\n",
    "# Set environment variables for psql connection\n",
    "os.environ[\"PGHOST\"] = cluster_ip_address\n",
    "os.environ[\"PGUSER\"] = database_user\n",
    "os.environ[\"PGPASSWORD\"] = database_password\n",
    "\n",
    "# Establish a connection to the database\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=cluster_ip_address,\n",
    "        user=database_user,\n",
    "        password=database_password\n",
    "    )\n",
    "    print(\"Connected to the database successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Connection error:\", e)\n",
    "exit(1)\n",
    "\n",
    "# Read the dataset from the URL\n",
    "DATASET_URL = \"https://github.com/GoogleCloudPlatform/python-docs-samples/raw/main/cloud-sql/postgres/pgvector/data/retail_toy_dataset.csv\"\n",
    "df = pd.read_csv(DATASET_URL)\n",
    "\n",
    "# Select desired columns and drop missing values\n",
    "df = df.loc[:, [\"product_id\", \"product_name\", \"description\", \"list_price\"]]\n",
    "df = df.dropna()\n",
    "\n",
    "# Save the DataFrame to the AlloyDB cluster\n",
    "df.to_sql('products', con=f'postgresql://{cluster_ip_address}', if_exists='replace', index=False)\n",
    "\n",
    "# Retrieve data from the 'products' table\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT * FROM products\")\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "print(results[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859ab7a-3065-4bde-b27c-be46e6e75c74",
   "metadata": {},
   "source": [
    "# Generate Vector Embeddings using a Text Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081ba186-4169-4d2a-9c74-ba9d1866abd8",
   "metadata": {},
   "source": [
    "In this section, let's **preprocess** \n",
    "- product descriptions, \n",
    "- generate vector embeddings for them, \n",
    "- and store the embeddings along with other relevant data in a PostgreSQL database table for downstream analysis or applications.\n",
    "\n",
    "Run the following code snippet to import the `RecursiveTextSplitter` class from the `LangChain library`, which is used for splitting text into smaller chunks. \n",
    "\n",
    "Iterate through each row in the DataFrame `df` and extract the **product ID** and **description** from each row.\n",
    "\n",
    "Then, we will split each description into smaller chunks and will create a dictionary for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a71caf8-e668-4c5b-a8db-0d77e9065b30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Set up the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\".\", \"\\n\"],\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Define the maximum number of documents to process\n",
    "max_documents = 50  # Reduced limit to further control API usage\n",
    "documents = []\n",
    "\n",
    "# Create Document objects with product_id as metadata\n",
    "for index, row in df.iterrows():\n",
    "    product_id = row[\"product_id\"]\n",
    "    desc = row[\"description\"]\n",
    "    documents.append(Document(page_content=desc, metadata={\"product_id\": product_id}))\n",
    "\n",
    "# Use the text splitter on a subset of documents (e.g., 40-50)\n",
    "chunked = []\n",
    "docs = text_splitter.split_documents(documents[40:max_documents])\n",
    "\n",
    "# Collect split content along with product_id\n",
    "for doc in docs:\n",
    "    chunked.append({\"product_id\": doc.metadata[\"product_id\"], \"content\": doc.page_content})\n",
    "\n",
    "print(len(chunked))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243eae7-8082-4ac0-aff6-c3efd9575225",
   "metadata": {},
   "source": [
    "Run the following code snippet to process product descriptions from a dataset by splitting them into smaller chunks, \n",
    "sending them to Vertex AI for embedding generation, and storing the retrieved embeddings back into the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fac86c9c-4539-4b72-a866-794d1a044ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>content</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8a6d71be41e01b284294ec488508b414</td>\n",
       "      <td>All of our productsWalmartply with internation...</td>\n",
       "      <td>[0.04086191579699516, -0.020511694252490997, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8a6d71be41e01b284294ec488508b414</td>\n",
       "      <td>. Holds Up to 6 Decks Fun for the whole family...</td>\n",
       "      <td>[0.01160380244255066, -0.021363208070397377, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9648838f5badebb9fc0b07f89cc29394</td>\n",
       "      <td>Better circulate water through your pool with ...</td>\n",
       "      <td>[-0.005303527694195509, 0.017071831971406937, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9648838f5badebb9fc0b07f89cc29394</td>\n",
       "      <td>.25-inch fitting (11070), 2 strainer grids (11...</td>\n",
       "      <td>[-0.01586345210671425, 0.017979448661208153, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9648838f5badebb9fc0b07f89cc29394</td>\n",
       "      <td>. Circulate water through your pool with the h...</td>\n",
       "      <td>[0.0184821505099535, 0.01372526679188013, 0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         product_id  \\\n",
       "0  8a6d71be41e01b284294ec488508b414   \n",
       "1  8a6d71be41e01b284294ec488508b414   \n",
       "2  9648838f5badebb9fc0b07f89cc29394   \n",
       "3  9648838f5badebb9fc0b07f89cc29394   \n",
       "4  9648838f5badebb9fc0b07f89cc29394   \n",
       "\n",
       "                                             content  \\\n",
       "0  All of our productsWalmartply with internation...   \n",
       "1  . Holds Up to 6 Decks Fun for the whole family...   \n",
       "2  Better circulate water through your pool with ...   \n",
       "3  .25-inch fitting (11070), 2 strainer grids (11...   \n",
       "4  . Circulate water through your pool with the h...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.04086191579699516, -0.020511694252490997, -...  \n",
       "1  [0.01160380244255066, -0.021363208070397377, -...  \n",
       "2  [-0.005303527694195509, 0.017071831971406937, ...  \n",
       "3  [-0.01586345210671425, 0.017979448661208153, -...  \n",
       "4  [0.0184821505099535, 0.01372526679188013, 0.00...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from google.cloud import aiplatform\n",
    "import time\n",
    "\n",
    "embeddings_service = VertexAIEmbeddings(model_name=\"text-embedding-005\")\n",
    "\n",
    "# embeddings_service = VertexAIEmbeddings(\n",
    "#     model_name=\"textembedding-gecko@latest\",  # or @003 if required\n",
    "#     project=\"qwiklabs-gcp-01-0205e348a520\",                # optional if default is set\n",
    "#     location=\"us-central1\"                    # must be explicitly set\n",
    "# )\n",
    "\n",
    "\n",
    "# Helper function to retry failed API requests with exponential backoff.\n",
    "def retry_with_backoff(func, *args, retry_delay=10, backoff_factor=2.5, **kwargs):  # Increased delay and backoff factor\n",
    "    max_attempts = 10\n",
    "    retries = 0\n",
    "    for i in range(max_attempts):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"error: {e}\")\n",
    "            retries += 1\n",
    "            wait = retry_delay * (backoff_factor**retries)\n",
    "            print(f\"Retry after waiting for {wait} seconds...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "# Reduced batch size for API calls to manage quota limits\n",
    "batch_size = 3\n",
    "for i in range(0, len(chunked), batch_size):\n",
    "    request = [x[\"content\"] for x in chunked[i : i + batch_size]]\n",
    "    response = retry_with_backoff(embeddings_service.embed_documents, request)\n",
    "    # Store the retrieved vector embeddings for each chunk back.\n",
    "    for x, e in zip(chunked[i : i + batch_size], response):\n",
    "        x[\"embedding\"] = e\n",
    "\n",
    "# Store the generated embeddings in a pandas dataframe.\n",
    "product_embeddings = pd.DataFrame(chunked)\n",
    "product_embeddings.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98948b06-6f19-4705-bf90-7fd32f69b317",
   "metadata": {},
   "source": [
    "Run the following command to enable AlloyDB integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e25ee783-94d4-417d-8925-9c7cdb13b29c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated IAM policy for project [qwiklabs-gcp-01-0205e348a520].\n",
      "bindings:\n",
      "- members:\n",
      "  - serviceAccount:service-622450730797@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-622450730797@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-622450730797@gcp-sa-alloydb.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.user\n",
      "- members:\n",
      "  - serviceAccount:service-622450730797@gcp-sa-alloydb.iam.gserviceaccount.com\n",
      "  role: roles/alloydb.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:qwiklabs-gcp-01-0205e348a520@qwiklabs-gcp-01-0205e348a520.iam.gserviceaccount.com\n",
      "  role: roles/bigquery.admin\n",
      "- members:\n",
      "  - serviceAccount:622450730797@cloudbuild.gserviceaccount.com\n",
      "  role: roles/cloudbuild.builds.builder\n",
      "- members:\n",
      "  - serviceAccount:service-622450730797@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
      "  role: roles/cloudbuild.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-622450730797@compute-system.iam.gserviceaccount.com\n",
      "  role: roles/compute.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-622450730797@container-engine-robot.iam.gserviceaccount.com\n",
      "  role: roles/container.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:622450730797-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:622450730797@cloudservices.gserviceaccount.com\n",
      "  - user:student-00-e9cf8a85d872@qwiklabs.net\n",
      "  role: roles/editor\n",
      "- members:\n",
      "  - serviceAccount:service-622450730797@gcp-sa-notebooks.iam.gserviceaccount.com\n",
      "  role: roles/notebooks.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:admiral@qwiklabs-services-prod.iam.gserviceaccount.com\n",
      "  - serviceAccount:qwiklabs-gcp-01-0205e348a520@qwiklabs-gcp-01-0205e348a520.iam.gserviceaccount.com\n",
      "  role: roles/owner\n",
      "- members:\n",
      "  - serviceAccount:622450730797-compute@developer.gserviceaccount.com\n",
      "  - user:student-00-e9cf8a85d872@qwiklabs.net\n",
      "  role: roles/resourcemanager.projectIamAdmin\n",
      "- members:\n",
      "  - serviceAccount:service-622450730797@service-networking.iam.gserviceaccount.com\n",
      "  role: roles/servicenetworking.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:qwiklabs-gcp-01-0205e348a520@qwiklabs-gcp-01-0205e348a520.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-622450730797@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
      "  role: roles/storage.admin\n",
      "- members:\n",
      "  - user:student-00-e9cf8a85d872@qwiklabs.net\n",
      "  role: roles/viewer\n",
      "etag: BwY2PreYf3A=\n",
      "version: 1\n"
     ]
    }
   ],
   "source": [
    "!PROJECT_ID=$(gcloud config get-value project) && \\\n",
    "PROJECT_NUMBER=$(gcloud projects list --filter=\"name=$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\") && \\\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "--member=\"serviceAccount:service-$PROJECT_NUMBER@gcp-sa-alloydb.iam.gserviceaccount.com\" \\\n",
    "--role=\"roles/aiplatform.user\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cdc2e2-defc-42e5-b1f0-53e3825f15df",
   "metadata": {},
   "source": [
    "Back in the AlloyDB service page, click on the cluster named cymbal-alloy-cluster, then select AlloyDB Studio from the left-hand side menu, enter the following values to sign in, and click on AUTHENTICATE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a366a3-ddf7-4a0b-8bea-c83598d05883",
   "metadata": {},
   "source": [
    "Field -> Value\n",
    "\n",
    "Database -> postgres\n",
    "\n",
    "User -> postgres\n",
    "\n",
    "Password -> postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510469b-ea4b-4d62-9546-7a8a69b27aec",
   "metadata": {},
   "source": [
    "n the AlloyDB Studio, click on Editor tab at the top.\n",
    "\n",
    "Enter the following command in the editor to grant the postgres user permission to execute the embedding function, install the google_ml_integration extension, and generate an embedding for the provided text using the textembedding-gecko model. Then, click on Run button at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dfaeeb-feac-47b0-9df8-03c3895a26bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRANT EXECUTE ON FUNCTION embedding TO postgres;\n",
    "\n",
    "CREATE EXTENSION IF NOT EXISTS google_ml_integration CASCADE;\n",
    "\n",
    "SELECT embedding('text-embedding-005', 'AlloyDB is a managed, cloud-hosted SQL database service.');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa292bff-08a9-4625-b489-4ae8f38302e4",
   "metadata": {},
   "source": [
    "Once the embeddings are successfully generated, click the Clear button at the top to clear the contents of the editor. Then, run the following query in the editor to prepare the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4f2f0-b82e-42df-90c2-b5b87dad64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE EXTENSION IF NOT EXISTS vector;\n",
    "\n",
    "DROP TABLE IF EXISTS product_embeddings;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa58bfb2-19ec-4c27-90ac-bb1443c0aa84",
   "metadata": {},
   "source": [
    "click the Clear button at the top to clear the contents of the editor. Run the following query, to create the embeddings based on product descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a110a-1254-4d08-a558-3da2b90aa0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE product_embeddings(\n",
    "        product_id VARCHAR(1024) NOT NULL PRIMARY KEY,\n",
    "        content TEXT,\n",
    "        embedding vector(768)\n",
    "    );\n",
    "\n",
    "\n",
    "insert into product_embeddings(product_id, content, embedding)\n",
    "SELECT\n",
    "product_id,\n",
    "description as content,\n",
    "embedding('text-embedding-005', description) as embedding\n",
    "from products\n",
    "where product_id not in (select product_id from product_embeddings)\n",
    "limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3534773-e0c9-4b35-b2b1-518ed7dcc56d",
   "metadata": {},
   "source": [
    "# Create Indexes for faster Similarity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee5d89-bc8d-420d-b771-3f1e7832dbfc",
   "metadata": {},
   "source": [
    "Vector indexes can significantly speed up similarity search operations and avoid the brute-force exact nearest neighbor search that is used by default.\n",
    "\n",
    "**Pgvector** comes with two types of indexes: **hnsw** and **ivfflat**.\n",
    "\n",
    "In the Editor, run the following query to build the **HNSW index** on **product_embeddings** table using cosine similarity metric for faster search based on descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a4563-cbba-476b-b1e2-6c48479d9b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create an HNSW index on the `product_embeddings` table\n",
    "CREATE INDEX ON product_embeddings\n",
    "USING hnsw(embedding vector_cosine_ops)\n",
    "WITH (m = 24, ef_construction = 100);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2d948-0882-4f8e-9701-ec8920a33a2c",
   "metadata": {},
   "source": [
    "Next, run the query to create an **IVFFLAT** index on the **product_embeddings** table using cosine similarity for swift similarity searches among product descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc428d-6ea0-431a-be8a-dbcc291220e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create an IVFFLAT index on the `product_embeddings` table\n",
    "CREATE INDEX ON product_embeddings\n",
    "USING ivfflat(embedding vector_cosine_ops)\n",
    "WITH (lists = 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fba80c-e6ba-40b6-9a56-fa167ab39cf7",
   "metadata": {},
   "source": [
    "Now, we will conduct the similarity search. The provided code identifies products most relevant to the user's query based on their textual descriptions. To ensure relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58d328-b402-4f40-85f6-abec0036bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with e as (\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    product_embeddings\n",
    "ORDER BY\n",
    "    embedding <-> CAST(embedding('text-embedding-005','Playing card games') AS vector(768)) asc\n",
    "LIMIT\n",
    "    5\n",
    ")\n",
    "select\n",
    "*\n",
    "from products\n",
    "where product_id in (select e.product_id from e);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a581ced-273b-4436-b4f0-3f4ed71f4be1",
   "metadata": {},
   "source": [
    "Finally, the top matches are displayed as a list, making it easy to find products that fit your search and budget. You'll see the product name, price, and a brief description for each result, giving you a quick overview of your options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a28564d-367b-47ce-ad42-3309127f4319",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LLMs and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9b0516-a21d-408d-ba89-61c5faddf6a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use case 1: Building an AI-curated contextual hybrid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318a01c-0cdf-4a82-9408-f0fdc9b6f3c4",
   "metadata": {},
   "source": [
    "\n",
    "Combine natural language query text with regular relational filters to create a powerful hybrid search.\n",
    "\n",
    "Build a user query with english text and the price filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3193d2eb-3d33-4077-89a3-5d930ed6f300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please fill in these values.\n",
    "user_query = \"Do you have a toy set that teaches numbers and letters to kids?\"  # @param {type:\"string\"}\n",
    "min_price = 10  # @param {type:\"integer\"}\n",
    "max_price = 100  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6a50fd-f216-4301-96f6-0cb5f167524c",
   "metadata": {},
   "source": [
    "Generate the vector embedding for the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4388ef33-eee9-4ffd-b9ea-fd815549693e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qe = embeddings_service.embed_query(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057beb61-8124-4aa4-8b91-755662a22b56",
   "metadata": {},
   "source": [
    "Use pgvector to find similar products. The pgvector similarity search operators provide powerful semantics to combine the vector search operation with regular query filters in a single SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "068a9773-ce8e-4cd8-a286-5a346984effa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results found. Try adjusting the similarity threshold or checking the data.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import pandas as pd\n",
    "\n",
    "def main(user_query, min_price, max_price):\n",
    "    try:\n",
    "        # AlloyDB cluster connection details (replace with your actual values)\n",
    "        cluster_ip_address = \"10.12.0.2\"\n",
    "        database_user = \"postgres\"\n",
    "        database_password = \"postgres\"\n",
    "\n",
    "        # Connect to AlloyDB cluster\n",
    "        conn = psycopg2.connect(\n",
    "            host=cluster_ip_address,\n",
    "            user=database_user,\n",
    "            password=database_password\n",
    "        )\n",
    "\n",
    "        # Register the vector type\n",
    "        register_vector(conn)\n",
    "\n",
    "        # Get the query embedding\n",
    "        qe = embeddings_service.embed_query(user_query)\n",
    "\n",
    "        # Check if qe is valid\n",
    "        if not qe:\n",
    "            print(\"Error: The query embedding is empty.\")\n",
    "            return\n",
    "\n",
    "        # Perform the similarity search and filtering\n",
    "        cur = conn.cursor()\n",
    "        similarity_threshold = 0.5  # Increased threshold for broader matching\n",
    "        num_matches = 50\n",
    "\n",
    "        # Modify the SQL query for indexed similarity search\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            WITH vector_matches AS (\n",
    "                SELECT product_id, embedding <=> %s::vector AS distance\n",
    "                FROM product_embeddings\n",
    "                WHERE embedding <=> %s::vector < %s\n",
    "                ORDER BY distance ASC\n",
    "                LIMIT %s\n",
    "            )\n",
    "            SELECT product_name, list_price, description\n",
    "            FROM products\n",
    "            WHERE product_id IN (SELECT product_id FROM vector_matches)\n",
    "            AND list_price >= %s AND list_price <= %s\n",
    "            \"\"\",\n",
    "            (qe, qe, similarity_threshold, num_matches, min_price, max_price)\n",
    "        )\n",
    "        results = cur.fetchall()\n",
    "\n",
    "        # Check if any results are retrieved\n",
    "        if not results:\n",
    "            print(\"No results found. Try adjusting the similarity threshold or checking the data.\")\n",
    "            return\n",
    "\n",
    "        # Process the results\n",
    "        matches = []\n",
    "        for r in results:\n",
    "            try:\n",
    "                list_price = round(float(r[1]), 2)  # Attempt conversion and rounding\n",
    "            except ValueError:\n",
    "                list_price = r[1]  # Use original value if conversion fails\n",
    "            matches.append({\n",
    "                \"product_name\": r[0],\n",
    "                \"list_price\": list_price,\n",
    "                \"description\": r[2]\n",
    "            })\n",
    "\n",
    "        # Display the results\n",
    "        matches_df = pd.DataFrame(matches)\n",
    "        print(matches_df.head(5))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during database operations: {e}\")\n",
    "    finally:\n",
    "        # Close the connection\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Call the main function\n",
    "main(\"Do you have a toy set that teaches numbers and letters to kids?\", 25, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a2b53d-d50f-4c91-974d-92b556cd0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Result Is Different than the Lab Result , Could be cause by using a different Text Embedding Model, Not Sure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b5390-a06d-4b42-9ce2-f22367e5dbf5",
   "metadata": {},
   "source": [
    "Use LangChain to summarize and generate a high-quality prompt to answer the user query.\n",
    "\n",
    "After finding the similar products and their descriptions using pgvector, the next step is to use them for generating a prompt input for the LLM model. Since individual product descriptions can be very long, they may not fit within the specified input payload limit for an LLM model. The MapReduceChain from the LangChain framework is used to generate and combine short summaries of similarly matched products. The combined summaries are then used to build a high-quality prompt for an input to the LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f851137-fe4a-4401-87bc-d39769e393b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's the answer based on the provided toy descriptions:\n",
       "\n",
       "1.  The toy set that teaches numbers and letters to kids is the **Alphabet Learning Toy**.\n",
       "2.  The price of the Alphabet Learning Toy is 30.\n",
       "3.  The Alphabet Learning Toy teaches letters and numbers.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Mock matches data\n",
    "matches = [\n",
    "    {\"product_name\": \"Alphabet Learning Toy\", \"price\": 30, \"features\": \"Teaches letters and numbers.\"},\n",
    "    {\"product_name\": \"Number Puzzle\", \"price\": 20, \"features\": \"Interactive puzzle for number learning.\"},\n",
    "]\n",
    "\n",
    "# LangChain setup\n",
    "llm = VertexAI(model_name=\"gemini-2.0-flash-001\")\n",
    "\n",
    "map_prompt_template = \"\"\"\n",
    "            You will be given a detailed description of a toy product.\n",
    "            This description is enclosed in triple backticks (```).\n",
    "            Using this description only, extract the name of the toy,\n",
    "            the price of the toy and its features.\n",
    "\n",
    "            ```{text}```\n",
    "            SUMMARY:\n",
    "            \"\"\"\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "                You will be given a detailed description of different toy products\n",
    "                enclosed in triple backticks (```) and a question enclosed in\n",
    "                double backticks(``).\n",
    "                Select one toy that is most relevant to answer the question.\n",
    "                Using that selected toy description, answer the following\n",
    "                question in as much detail as possible.\n",
    "                You should only use the information in the description.\n",
    "                Your answer should include the name of the toy, the price of the toy\n",
    "                and its features. Your answer should be less than 200 words.\n",
    "                Your answer should be in Markdown in a numbered list format.\n",
    "\n",
    "                Description:\n",
    "                ```{text}```\n",
    "\n",
    "                Question:\n",
    "                ``{user_query}``\n",
    "\n",
    "                Answer:\n",
    "                \"\"\"\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"text\", \"user_query\"]\n",
    ")\n",
    "\n",
    "# Convert matches to LangChain documents\n",
    "docs = [\n",
    "    Document(page_content=f\"Name: {match['product_name']}, Price: {match['price']}, Features: {match['features']}\")\n",
    "    for match in matches\n",
    "]\n",
    "\n",
    "# Load and invoke the chain\n",
    "chain = load_summarize_chain(\n",
    "    llm, chain_type=\"map_reduce\", map_prompt=map_prompt, combine_prompt=combine_prompt\n",
    ")\n",
    "\n",
    "# User query\n",
    "user_query = \"Do you have a toy set that teaches numbers and letters to kids?\"\n",
    "\n",
    "# Invoke the chain\n",
    "output = chain.invoke({\n",
    "    \"input_documents\": docs,\n",
    "    \"user_query\": user_query,\n",
    "})\n",
    "\n",
    "# Extract and display the output\n",
    "answer = output.get('output_text', ' ')\n",
    "display(Markdown(answer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4c8391-fd53-4158-a621-43a3b926f621",
   "metadata": {},
   "source": [
    "### Use case 2: Adding AI-powered creative content generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a01f1b5-a009-4950-b72c-a2e82e3f45d1",
   "metadata": {},
   "source": [
    "Use knowledge from the existing dataset to generate new AI-powered content from an initial prompt.\n",
    "\n",
    "A third-party seller on the retail platform wants to use the AI-powered content generation to create a detailed description of their new bicycle product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b04a1ca-f6c0-46d3-a587-d6f828524dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please fill in these values.\n",
    "creative_prompt = \"A bicycle with brand name 'Roadstar bike' for kids that comes with training wheels and helmet.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf47eb8-f762-43a7-b89a-4489c1d93786",
   "metadata": {
    "tags": []
   },
   "source": [
    "Leverage the pgvector similarity search operator to find an existing product description that closely matches the new product specified in the initial prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87e98c4b-e286-4dc5-afb2-d31360e17b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches found: ['Turn any small bicycle into an instrument for learning to ride with the Schwinn 12\"-20\" Training Wheels. They feature a slotted design to fit 12\" to 20\" bikes. The training wheels are easy to assemble, install and remove, so that when your little one is able to ride without assistance, you can take them off. These bicycle training wheels include steel brackets and rubber tires that can stand up to heavy use. Training Wheels, Fits 12 inches - 20 inches bicycles. Est. 1895. Durable Construction: Steel brackets stand up to heavy use. Customizable: Two sets of wheel decals included. Features: Fits Most Childrens Bicycles: Intended for 12 inch - 20 inch bicycles. Steel Brackets: Offer increased durability. Includes two sets of wheel decals: Learn how to ride in style - see images below. Easy to Adjust: Slotted design for size adjustment. Includes: One pair of training wheels, four decals, installation instructions, and all mounting hardware. Tools required: Adjustable wrench. www.schwinnbikes.com. Follow ride Schwinn on: Twitter. Facebook. Made in China. Training Wheels,Fits 12 inches - 20 inches bicycles. Est. 1895. Durable Construction: Steel brackets stand up to heavy use. Customizable: Two sets of wheel decals included. Features: Fits Most Childrens Bicycles: Intended for 12 inch - 20 inch bicycles. Steel Brackets: Offer increased durability. Includes two sets of wheel decals: Learn how to ride in style - see images below. Easy to Adjust: Slotted design for size adjustment. Includes: One pair of training wheels, four decals, installation instructions, and all mounting hardware. Tools required: Adjustable wrench. www.schwinnbikes.com. Follow ride Schwinn on: Twitter. Facebook. Made in China.']\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # AlloyDB cluster connection details\n",
    "        cluster_ip_address = \"10.12.0.2\"\n",
    "        database_user = \"postgres\"\n",
    "        database_password = \"postgres\"\n",
    "\n",
    "        # Connect to AlloyDB cluster\n",
    "        conn = psycopg2.connect(\n",
    "            host=cluster_ip_address,\n",
    "            user=database_user,\n",
    "            password=database_password\n",
    "        )\n",
    "\n",
    "        # Register the vector type\n",
    "        register_vector(conn)\n",
    "\n",
    "        # Get the query embedding\n",
    "        qe = embeddings_service.embed_query(creative_prompt)\n",
    "\n",
    "        # Check if qe is a valid embedding\n",
    "        if not qe:\n",
    "            print(\"Error: The query embedding is empty.\")\n",
    "            return\n",
    "\n",
    "        # Set similarity threshold\n",
    "        similarity_threshold = 0.5\n",
    "        matches = []\n",
    "\n",
    "        # Perform the similarity search and filtering\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            WITH vector_matches AS (\n",
    "                SELECT product_id, embedding <=> %s::vector AS distance\n",
    "                FROM product_embeddings\n",
    "                WHERE embedding <=> %s::vector < %s\n",
    "                ORDER BY distance ASC\n",
    "                LIMIT 1\n",
    "            )\n",
    "            SELECT description FROM products\n",
    "            WHERE product_id IN (SELECT product_id FROM vector_matches)\n",
    "            \"\"\",\n",
    "            (qe, qe, similarity_threshold)\n",
    "        )\n",
    "\n",
    "        results = cur.fetchall()\n",
    "\n",
    "        # Process the results\n",
    "        for r in results:\n",
    "            matches.append(r[0])\n",
    "\n",
    "        if not matches:\n",
    "            print(\"No matches found.\")\n",
    "        else:\n",
    "            print(\"Matches found:\", matches)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during database operations: {e}\")\n",
    "    finally:\n",
    "        # Close the connection if it was established\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Call the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f6964b-80f2-4bae-8ff6-904c3aac5444",
   "metadata": {},
   "source": [
    "Use the existing matched product description as the prompt context to generate new creative output from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9394b413-0611-4c56-bc8d-a4d461f5df7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a toy description suitable for both indoor and outdoor play, combining elements from toy descriptions 1 & 2:\n",
       "\n",
       "*   **The \"AdventureSphere\":** A large, inflatable ball made of durable, weather-resistant material.\n",
       "\n",
       "*   **Versatile Play:** Perfect for rolling, bouncing, and chasing in the backyard, park, or even inside on a rainy day.\n",
       "\n",
       "*   **Imaginative Designs:** Available in various vibrant colors and patterns, including glow-in-the-dark options for evening fun. Some models feature printed maps or constellations to spark curiosity.\n",
       "\n",
       "*   **Safe and Durable:** Constructed with reinforced seams and a secure valve to prevent leaks. Phthalate-free and non-toxic.\n",
       "\n",
       "*   **Interactive Features:** Certain AdventureSpheres include built-in sound effects or light-up elements activated by motion, adding an extra layer of engagement.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# Define the template\n",
    "template = \"\"\"\n",
    "            You are given descriptions about some similar kind of toys in the context.\n",
    "            This context is enclosed in triple backticks (```).\n",
    "            Combine these descriptions and adapt them to match the specifications in\n",
    "            the initial prompt. All the information from the initial prompt must\n",
    "            be included. You are allowed to be as creative as possible,\n",
    "            and describe the new toy in as much detail. Your answer should be\n",
    "            in markdown in lists and less than 200 words.\n",
    "\n",
    "            Context:\n",
    "            ```{context}```\n",
    "\n",
    "            Initial Prompt:\n",
    "            {creative_prompt}\n",
    "\n",
    "            Answer:\n",
    "        \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"context\", \"creative_prompt\"]\n",
    ")\n",
    "\n",
    "# Define the LLM\n",
    "llm = VertexAI(model_name=\"gemini-2.0-flash-001\", temperature=0.7)\n",
    "\n",
    "# Example `matches` list\n",
    "matches = [\n",
    "    {\"description\": \"This is a toy description 1.\"},\n",
    "    {\"description\": \"This is a toy description 2.\"},\n",
    "    {},  # Missing `description`\n",
    "    \"Invalid item\"  # Not a dictionary\n",
    "]\n",
    "\n",
    "# Construct the context by extracting valid descriptions\n",
    "context = \"\\n\".join(\n",
    "    item[\"description\"] for item in matches if isinstance(item, dict) and \"description\" in item\n",
    ")\n",
    "\n",
    "# Define the creative prompt\n",
    "creative_prompt = \"Describe a toy that is suitable for both indoor and outdoor play.\"\n",
    "\n",
    "# Use RunnableSequence for chaining\n",
    "llm_chain = RunnableSequence(prompt | llm)\n",
    "\n",
    "# Invoke the chain\n",
    "answer = llm_chain.invoke({\n",
    "    \"context\": context,\n",
    "    \"creative_prompt\": creative_prompt,\n",
    "})\n",
    "\n",
    "# Display the answer in Markdown format\n",
    "display(Markdown(answer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12283f-4a22-4b44-a881-f2b17377c48f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
